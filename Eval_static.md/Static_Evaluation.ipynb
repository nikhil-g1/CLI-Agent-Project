{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IxqYeeOUi9ld"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from peft import PeftModel\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge import Rouge\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test prompts and two edge cases\n",
        "test_prompts = [\n",
        "    #Official prompts from Section 7\n",
        "    \"Create a new Git branch and switch to it.\",\n",
        "    \"Compress the folder reports into reports.tar.gz.\",\n",
        "    \"List all Python files in the current directory recursively.\",\n",
        "    \"Set up a virtual environment and install requests.\",\n",
        "    \"Fetch only the first ten lines of a file named output.log.\",\n",
        "    #Custom edge cases\n",
        "    \"Find and kill all processes using more than 1GB of memory.\",\n",
        "    \"Schedule a cron job to run cleanup.sh every Sunday at 2AM.\"\n",
        "]"
      ],
      "metadata": {
        "id": "MARSQgbxln3j"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining model and adapter path\n",
        "base_model_id = \"Qwen/Qwen2-0.5B\"\n",
        "adapter_path = \"lora-qwen2-adapter\"\n",
        "\n",
        "#Loading tokenizer & base model\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
        "gen_base = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer)\n",
        "\n",
        "#Loading fine-tuned model\n",
        "tuned_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "gen_tuned = pipeline(\"text-generation\", model=tuned_model, tokenizer=tokenizer)\n",
        "\n",
        "#Generate responses\n",
        "results =[]\n",
        "for prompt in test_prompts:\n",
        "  base_out = gen_base(f\"Step-by-step plan for: {prompt}\\n\", max_new_tokens=100)[0][\"generated_text\"]\n",
        "  tuned_out = gen_tuned(f\"Step-by-step plan for: {prompt}\\n\", max_new_tokens=100)[0][\"generated_text\"]\n",
        "  results.append((prompt, base_out, tuned_out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFzNOuMTkETv",
        "outputId": "1d4d391d-ba25-4d7e-d604-c0f1efc82378"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading punkt\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUCAfoptsKJs",
        "outputId": "716a37ac-b589-4ca9-c184-b36a478ec82c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining scoring function first\n",
        "def score_plan_quality(response: str) -> int:\n",
        "    steps = response.count(\"\\n\")\n",
        "    if steps < 2:\n",
        "      return 0\n",
        "    if any(word in response.lower() for word in [\"step\", \"run\", \"command\", \"execute\"]):\n",
        "        return 2\n",
        "    return 1\n",
        "\n",
        "#Computing BLEU & ROUGE-L\n",
        "rouge = Rouge()\n",
        "bleu_scores = []\n",
        "rouge_scores = []\n",
        "plan_scores = []\n",
        "\n",
        "for prompt, base_out, tuned_out in results:\n",
        "  ref = word_tokenize(base_out)\n",
        "  hyp = word_tokenize(tuned_out)\n",
        "  bleu = sentence_bleu([ref], hyp)\n",
        "  rouge_score = rouge.get_scores(tuned_out, base_out)[0][\"rouge-l\"][\"f\"]\n",
        "  plan_quality = score_plan_quality(tuned_out)\n",
        "\n",
        "  bleu_scores.append(bleu)\n",
        "  rouge_scores.append(rouge_score)\n",
        "  plan_scores.append(plan_quality)\n",
        "\n",
        "print(\"\\nMetric Comparison\")\n",
        "for i, (prompt, base_out, tuned_out) in enumerate(results):\n",
        "  print(f\"Prompt {i+1}: {prompt}\")\n",
        "  print(f\"BLEU: {bleu_scores[i]:.2f} | ROUGE-L: {rouge_scores[i]:.2f}\")\n",
        "  print(f\"Plan quality: {plan_scores[i]} / 2\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltFcsPUBm9dd",
        "outputId": "317dc693-38b4-44ae-cfe3-7088fc66e5eb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metric Comparison\n",
            "Prompt 1: Create a new Git branch and switch to it.\n",
            "BLEU: 0.18 | ROUGE-L: 0.39\n",
            "Plan quality: 2 / 2\n",
            "\n",
            "Prompt 2: Compress the folder reports into reports.tar.gz.\n",
            "BLEU: 0.12 | ROUGE-L: 0.29\n",
            "Plan quality: 2 / 2\n",
            "\n",
            "Prompt 3: List all Python files in the current directory recursively.\n",
            "BLEU: 0.21 | ROUGE-L: 0.36\n",
            "Plan quality: 2 / 2\n",
            "\n",
            "Prompt 4: Set up a virtual environment and install requests.\n",
            "BLEU: 0.17 | ROUGE-L: 0.39\n",
            "Plan quality: 2 / 2\n",
            "\n",
            "Prompt 5: Fetch only the first ten lines of a file named output.log.\n",
            "BLEU: 0.04 | ROUGE-L: 0.42\n",
            "Plan quality: 0 / 2\n",
            "\n",
            "Prompt 6: Find and kill all processes using more than 1GB of memory.\n",
            "BLEU: 0.20 | ROUGE-L: 0.40\n",
            "Plan quality: 2 / 2\n",
            "\n",
            "Prompt 7: Schedule a cron job to run cleanup.sh every Sunday at 2AM.\n",
            "BLEU: 0.18 | ROUGE-L: 0.44\n",
            "Plan quality: 2 / 2\n",
            "\n"
          ]
        }
      ]
    }
  ]
}